{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SwayamParida/handwritten-math-exp-recognition/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MzZIZRy8mj9u",
    "outputId": "b9bb8c0a-a729-4fcf-8e8d-d1db58905843"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QI85gR0RV1jE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from skimage import feature\n",
    "from sklearn import linear_model, model_selection, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim, utils\n",
    "from torchvision import models, transforms\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36JRp90oV1jT"
   },
   "source": [
    "## Atomic Symbol Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZK3jzAVV1jV"
   },
   "source": [
    "The atomic symbol dataset is contained in the directory referenced by the relative filepath stored in `ATOMIC_SYMBOL_DATASET_DIR`. The dataset contains a subdirectory per math symbol with the directory name corresponding to the symbol name. Each subdirectory contains JPG images of the handwritten symbols that serve as training examples for that symbol class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NsnoAHhlmKvJ"
   },
   "outputs": [],
   "source": [
    "%cd drive/My\\ Drive/cs231n/project/\n",
    "!unzip -n data/data.zip -d data/handwritten-symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DN31FFF2V1jW"
   },
   "outputs": [],
   "source": [
    "ATOMIC_SYMBOL_DATASET_DIR = 'Data/Atomic-Handwritten-Math-Symbols'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xg0VFvpav9bp"
   },
   "outputs": [],
   "source": [
    "def print_progress(i, num_items):\n",
    "  progress = '%.2f' % (i / num_items * 100)\n",
    "  progress = f'{progress}% done.'\n",
    "  progress = ('\\b' * len(progress)) + progress\n",
    "  print(progress, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2dOBf5yV1jZ"
   },
   "outputs": [],
   "source": [
    "class AtomicSymbolDataset(utils.data.Dataset):\n",
    "  def __init__(self, root_dir, max_examples_per_class=None, transform=None):\n",
    "    self.root_dir = root_dir\n",
    "    self.max_examples_per_class = max_examples_per_class\n",
    "    self.transform = transform\n",
    "    self.size = 0\n",
    "    self.build_dataset_info()\n",
    "    self.num_classes = len(self.symbols)\n",
    "    self.label_encoder = preprocessing.LabelEncoder()\n",
    "    self.label_encoder.fit(list(self.symbols.keys()))\n",
    "\n",
    "  def build_dataset_info(self):\n",
    "    self.symbols = defaultdict(list)\n",
    "    for i, d in enumerate(os.listdir(self.root_dir)):\n",
    "      symbol_dir = os.path.join(self.root_dir, d)\n",
    "      if not os.path.isdir(symbol_dir): continue\n",
    "      for j, f in enumerate(os.listdir(symbol_dir)):\n",
    "        if not '.jpg' in f: continue\n",
    "        if j >= self.max_examples_per_class: break\n",
    "        self.symbols[d].append(os.path.join(symbol_dir, f))\n",
    "#       print_progress(i, len(os.listdir(self.root_dir)))\n",
    "      self.size += len(self.symbols[d])\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    for symbol, imgs in self.symbols.items():\n",
    "      if idx < len(imgs):\n",
    "        img = Image.open(imgs[idx]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "        return (img, self.label_encoder.transform([symbol])[0])\n",
    "      else:\n",
    "        idx -= len(imgs)\n",
    "    raise IndexError('Index out of bounds')\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "FBx0JjBxwMGO",
    "outputId": "eb1d4d1f-a59d-4652-ff25-f0dc67603e40"
   },
   "outputs": [],
   "source": [
    "symbol_dataset = AtomicSymbolDataset(ATOMIC_SYMBOL_DATASET_DIR, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hV-nREOeV1j6"
   },
   "source": [
    "Getting a feel for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSzgDfHjV1j7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: ['leq']\n",
      "Image dimensions: (45, 45)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.choice(len(symbol_dataset))\n",
    "sample_img, sample_img_label = symbol_dataset[random_index]\n",
    "sample_img_label = symbol_dataset.label_encoder.inverse_transform(sample_img_label)\n",
    "_ = plt.imshow(sample_img, cmap='Greys_r')\n",
    "print(f'Symbol: {sample_img_label}')\n",
    "print(f'Image dimensions: {sample_img.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Hcd8loK12GM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 67583\n",
      "Number of classes: 82\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of examples: {len(symbol_dataset)}')\n",
    "# print(f'Number of training examples: {X_train.shape[0]}')\n",
    "# print(f'Number of testing examples: {X_test.shape[0]}')\n",
    "print(f'Number of classes: {symbol_dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDKvB4QUV1ki"
   },
   "source": [
    "# Baseline 1 - Symbol Segmentation and Atomic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YcNQpAnV1kj"
   },
   "source": [
    "To establish a baseline for the classification task, we shall develop a rudimentary model that segments each image of a mathematical expression into individual symbols and then feed these symbols to a Softmax classifer that has been trained on the atomic symbols dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNIojJjmV1kk"
   },
   "source": [
    "Segmenting expressions into individual symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6xqM9jEV1kk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sv4ZaRW-V1kv"
   },
   "outputs": [],
   "source": [
    "def featurize(X, featurizers=None):\n",
    "  X_feat = []\n",
    "  for ex in X:\n",
    "    if featurizers is None:\n",
    "      x = np.ndarray.flatten(ex)\n",
    "    else:\n",
    "      x = np.concatenate([np.ndarray.flatten(featurizer(ex)) for featurizer in featurizers])\n",
    "    X_feat.append(x)\n",
    "  return np.array(X_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxOnapfQ37pR"
   },
   "source": [
    "Converting PyTorch Dataset object into 2D Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZhv6Ij734fZ"
   },
   "outputs": [],
   "source": [
    "X = np.empty(shape=(len(symbol_dataset), *symbol_dataset[0][0].shape))\n",
    "y = np.empty(shape=len(symbol_dataset), dtype=np.str)\n",
    "\n",
    "progress = str()\n",
    "for i in range(len(symbol_dataset)):\n",
    "  X[i], y[i] = symbol_dataset[i]\n",
    "  if i % 20 == 0: print_progress(i, len(symbol_dataset))\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rORCwYMiV1ks"
   },
   "source": [
    "Training a Softmax classifier on the raw pixels of the atomic symbols dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPuc1m1bV1kt"
   },
   "outputs": [],
   "source": [
    "softmax_classifier = linear_model.LogisticRegression()\n",
    "X_train_feat = featurize(X_train)\n",
    "softmax_classifier.fit(X_train_feat, y_train)\n",
    "\n",
    "train_acc = softmax_classifier.score(X_train_feat, y_train)\n",
    "test_acc = softmax_classifier.score(featurize(X_test), y_test)\n",
    "print(f'Train accuracy: {train_acc}')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bzw6uO0T46pZ"
   },
   "source": [
    "Training the classifier with DAISY feature descriptors instead of raw pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-71p_HDO6gAG"
   },
   "outputs": [],
   "source": [
    "X_train_feat = featurize(X_train, [feature.daisy])\n",
    "softmax_classifier.fit(featurize(X_train_feat), y_train)\n",
    "\n",
    "train_acc = softmax_classifier.score(X_train_feat, y_train)\n",
    "test_acc = softmax_classifier.score(featurize(X_test, [feature.daisy]), y_test)\n",
    "print(f'Train accuracy: {train_acc}')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJS4PJA5dIeu"
   },
   "source": [
    "Using AlexNet as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yd2z78tB6sW7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=82, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.alexnet(pretrained=True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "  param.requires_grad = False\n",
    "num_features = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_features, symbol_dataset.num_classes)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "print(model_ft)\n",
    "\n",
    "params_to_update = [param for param in model_ft.parameters() if param.requires_grad]\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm0O-MWgoinN"
   },
   "source": [
    "Perform image transformations so that dataset images fit AlexNet configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0W23g-OloiFg"
   },
   "outputs": [],
   "source": [
    "tsfm = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeC6Dr4LpcsA"
   },
   "outputs": [],
   "source": [
    "test_train_split = [0.8, 0.2]\n",
    "assert(np.sum(test_train_split) == 1.0)\n",
    "\n",
    "symbol_dataset = AtomicSymbolDataset(ATOMIC_SYMBOL_DATASET_DIR, 100, tsfm)\n",
    "\n",
    "split_lengths = np.round(np.multiply(test_train_split, len(symbol_dataset)))\n",
    "split_lengths = split_lengths.astype(int)\n",
    "assert(np.sum(split_lengths) == len(symbol_dataset))\n",
    "\n",
    "train_set, test_set = utils.data.random_split(symbol_dataset, split_lengths)\n",
    "train_loader = utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "test_loader = utils.data.DataLoader(test_set, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPIOeA9IgrDU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   250] loss: 4.212\n",
      "[1,   500] loss: 3.677\n",
      "[1,   750] loss: 3.704\n",
      "[1,  1000] loss: 3.729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a40d5fe46ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_ft.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_ft(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 250 == 249:    # print every 250 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 250))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSCCRdEWmT-D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit308fc767c2834ccd82281a8640407f24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
